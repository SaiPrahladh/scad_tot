# -*- coding: utf-8 -*-
"""RT-Network-v3.1.x.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MfD_C225OafgIsQKVy76p3E2Qam6kXDo
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
# %load_ext tensorboard

import glob, math, os, re, sys, zipfile
import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import pickle as pkl
import matplotlib.pyplot as plt
from functools import reduce
from itertools import cycle
from datetime import datetime
from google.colab import auth
from oauth2client.client import GoogleCredentials
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from IPython.display import display, HTML
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.utils import resample
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, InputLayer
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard

"""## Authorize Google Drive"""

auth.authenticate_user()

"""## Setup & Install
Basic setup and install additional dependencies
"""

# Some global variables and general settings
saved_model_dir = './saved_model'
tensorboard_logs = './logs'
pd.options.display.float_format = '{:.2f}'.format
sns.set_context('notebook')
nnet_tools_path = os.path.abspath('NNet')

def print_html(string, tag='span', color=None, size=None):
    size = f'font-size:{size};' if size else ''
    color = f'color:{color};' if color else ''
    display(HTML(f'<{tag} style="{color}{size}">{string}</{tag}>'))

def print_heading(string, color=None):
    print_html(string, tag='h3', color=color)

def print_message(string, color=None):
    print_html(string, color=color)

def download_file_from_gdrive(gdrive_id, output_file):
    # Authenticate google drive
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)
    # Download csv from GDrive...
    dataset = drive.CreateFile({'id': gdrive_id})
    dataset_filename = dataset['title']
    print_message(f'Downloading {dataset_filename} ({gdrive_id}) from GDrive. Please wait...')
    dataset.GetContentFile(output_file)
    print_message(f'Download {gdrive_id} completed.')

def download_and_unzip(src_url, out_dir='./', zip_file='dl.zip', remove_zip=True):
    print(f'Downloading {src_url} to {zip_file}')
    !wget $src_url -O $zip_file -q --show-progress
    print(f'Download complete. Unzipping {zip_file}')
    z = zipfile.ZipFile(zip_file, 'r')
    z.extractall(out_dir)
    print(f'Unzipped to {out_dir}. Cleaning up...')
    z.close()
    if remove_zip: os.remove(zip_file)

def overwrite_gdrive_file(gdrive_id, input_file):
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)
    drive_file = drive.CreateFile({'id': gdrive_id})
    drive_file.SetContentFile(input_file)
    drive_file.Upload()
    drive_filename = drive_file['title']
    print(f'Wrote {input_file} to GDrive {drive_filename} ({gdrive_id}).')

def update_path_vars(paths=[]):
    python_path = os.environ.get('PYTHONPATH') or ''
    jupyter_path = os.environ.get('JUPYTER_PATH') or ''
    for path in paths:
        if not path in python_path:
            python_path += f':{path}'
        if not path in jupyter_path:
            jupyter_path += f':{path}'
    os.environ['PYTHONPATH'] = python_path
    os.environ['JUPYTER_PATH'] = jupyter_path

def install_nnet_tools(nnet_tools_path):
    nnet_tools_url = 'https://github.com/sisl/NNet/archive/master.zip'
    download_and_unzip(nnet_tools_url)
    !mv ./NNet-master $nnet_tools_path

def setup_nnet_tools(nnet_tools_path):
    # install nnet tools if not already installed.
    if not os.path.exists(nnet_tools_path):
        install_nnet_tools(nnet_tools_path)
    # add folder to PYTHONPATH & JUPYTER_PATH
    update_path_vars([nnet_tools_path])

# delete sample data
!rm -rf sample_data

# setup nnet tools (for converting model to Stanford's nnet format)
setup_nnet_tools(nnet_tools_path)
# used for conversion to NNet format
from NNet.utils.writeNNet import writeNNet

"""## Download Dataset"""

# GDrive ID of csv file (AllData_ReactionTime.csv)
# https://drive.google.com/file/d/1vNT9PopDTy7nUsedAHdg1-VFAKrk6PIO
gdrive_id='1vNT9PopDTy7nUsedAHdg1-VFAKrk6PIO'
dataset_file='all_data_rt.csv'

# load the dataset from gdrive if it doesn't exist in the runtime's filesystem.
if not os.path.exists(dataset_file):
    download_file_from_gdrive(gdrive_id, dataset_file)

"""## Import Dataset"""

raw_columns = ['ID', 'Name', 'FixationDuration', 'FixationSeq', 'FixationStart', 
               'FixationX', 'FixationY', 'GazeDirectionLeftZ', 'GazeDirectionRightZ', 
               'PupilLeft', 'PupilRight', 'InterpolatedGazeX', 'InterpolatedGazeY', 
               'AutoThrottle', 'AutoWheel', 'CurrentThrottle', 'CurrentWheel', 
               'Distance3D', 'MPH', 'ManualBrake', 'ManualThrottle', 'ManualWheel', 
               'RangeW', 'RightLaneDist', 'RightLaneType', 'LeftLaneDist', 'LeftLaneType', 
               'ReactionTime']
raw_df = pd.read_csv(dataset_file, usecols=raw_columns)
raw_df.set_index(['ID'], inplace=True)

def create_tot_categories(rt_column):
    rt_mean = rt_column.mean()
    rt_sdev = rt_column.std()
    bound_1 = rt_mean - rt_sdev
    bound_2 = rt_mean + rt_sdev
    bins = [float('-inf'), bound_1, bound_2, float('inf')]
    labels =  np.array(['fast', 'normal', 'slow'], dtype=object)
    return (bins, labels)

# make a copy the raw data
df = raw_df.copy()

# compute 'TOT' categories
tot_bins, tot_labels = create_tot_categories(df.ReactionTime)

# add the class to the dataframe
df['TOT'] = pd.cut(df.ReactionTime, bins=tot_bins, labels=tot_labels)

# Select a handful of ppl for saving resource
chunk_users = ['015_M3', '015_m2', '015_M1', '014_M3', '014_M2', '014_m1']
unused_df = df.loc[~df['Name'].isin(chunk_users)]
df = df.loc[df['Name'].isin(chunk_users)]

# # convert datatypes for a few columns
df.RightLaneType = df.RightLaneType.astype(int)
df.LeftLaneType = df.LeftLaneType.astype(int)
df.TOT = df.TOT.astype(object)

def upsample_minority_TOTs(X_train, y_train, tot_labels, random_state=27):
    # contat the training data together.
    X = pd.concat([X_train, y_train], axis=1)
    # separate majority and minority classes
    buckets = {l: X[X.TOT == l] for l in tot_labels}
    maj_label, majority = reduce(lambda a,b: b if b[1].shape[0] > a[1].shape[0] else a, buckets.items())
    minorities = {k:v for k,v in buckets.items() if k != maj_label}
    # upsample the minority classes
    for k,v in minorities.items():
        buckets[k] = resample(v, replace=True, n_samples=majority.shape[0], random_state=random_state)
    upsampled = pd.concat(buckets.values()).sample(frac=1)
    # split the upsampled data into X and y
    y_train = upsampled['TOT']
    X_train = upsampled.drop('TOT', axis=1)
    return X_train, y_train

def prepare_inputs(X_train, X_test):
    # scales inputs using "standard scaler", and returns 2D numpy array
    scaler = StandardScaler().fit(pd.concat([X_train, X_test]))
    X_train = scaler.transform(X_train.values)
    X_test = scaler.transform(X_test.values)
    return X_train, X_test, scaler

def prepare_target(y_train, y_test, categories):
    # convert target to categorical, and returns 2D numpy array
    y_train = y_train.to_numpy().reshape(-1,1)
    y_test = y_test.to_numpy().reshape(-1,1)
    onehot = OneHotEncoder(categories=categories)
    onehot.fit(np.concatenate([y_train, y_test]))
    y_train = onehot.transform(y_train).toarray()
    y_test = onehot.transform(y_test).toarray()
    return y_train, y_test, onehot

# split features and targets
y = df.TOT
X = df.drop(['Name', 'ReactionTime', 'TOT'], axis=1)

# make results easier to reproduce
random_state = 27

# split train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=random_state)

# upsample the training data
X_train, y_train = upsample_minority_TOTs(X_train, y_train, tot_labels)

# scale the inputs
X_train_enc, X_test_enc, scaler = prepare_inputs(X_train, X_test)
# categorize outputs
y_train_enc, y_test_enc, onehot = prepare_target(y_train, y_test, categories=[tot_labels])

print_heading('TOT Value Counts')
print(y_train.value_counts())

# save the column names & indexes for use during verification
feature_names = list(X.columns)

# display the feature names
print_heading('Feature Names')
print_message(feature_names)

# print the TOT categories
print_heading('TOT Categories')
print('\n'.join(['%s: %7.2f, %7.2f' % (tot_labels[i].rjust(6), tot_bins[i], tot_bins[i+1]) for i in range(3)]))

def display_processed_data(unencoded=True, encoded=True):
    if unencoded:
        print_heading('Unencoded Data')
        display(pd.concat([X_train, y_train], axis=1))
    if encoded:
        enc_tot_labels = onehot.get_feature_names(input_features=['TOT'])
        print_heading('Encoded Data')
        display(pd.concat([pd.DataFrame(X_train_enc, columns=X_train.columns),
                           pd.DataFrame(y_train_enc, columns=enc_tot_labels)],
                          axis=1).astype({k:int for k in enc_tot_labels}))
display_processed_data()

"""## Build & Train NN"""

# cleanup the old training logs and models
!rm -rf $tensorboard_logs model-*.h5 $saved_model_dir

# training callbacks
mc_file = 'model-best-{epoch:02d}-{val_loss:.2f}.h5'
es_cb = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)
mc_cb = ModelCheckpoint(mc_file, monitor='val_loss', mode='min', verbose=1, save_best_only=True)
tb_cb = TensorBoard(log_dir=tensorboard_logs, histogram_freq=1, write_graph=True, write_images=True)

# build neural network

# v3.1.3 (23,14,8): accuracy ~0.895, loss ~0.230
# model = Sequential()
# model.add(InputLayer(input_shape=(X_train_enc.shape[1],)))
# model.add(Dense(23, activation='relu', kernel_initializer='he_normal'))
# model.add(Dense(14, activation='relu'))
# model.add(Dense(8, activation='relu'))
# model.add(Dense(3, activation='softmax')) # logits layer

# v3.1.4 (28,14,8): accuracy ~0.9124, loss ~0.2079
model = Sequential()
model.add(InputLayer(input_shape=(X_train_enc.shape[1],)))
model.add(Dense(28, activation='relu', kernel_initializer='he_normal'))
model.add(Dense(14, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(3, activation='softmax')) # logits layer

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# fit the keras model on the dataset
history_3 = model.fit(X_train_enc, y_train_enc,
                      validation_split=0.10,
                      epochs=50,
                      batch_size=16,
                      callbacks=[es_cb, mc_cb, tb_cb])

# pick best model file from filesystem
best_model_path = sorted(glob.glob('model-best-*.h5'), key=lambda f: int(re.search(r'\d+', f).group()))[-1]

print_heading('Best Model:')
print_message(best_model_path)

# cleanup old model
!rm -rf $saved_model_dir

# save model in tf and h5 formats
tf_model_path = f'{saved_model_dir}/model'
h5_model_path = f'{saved_model_dir}/model.h5'
model.save(tf_model_path, save_format='tf')
model.save(h5_model_path, save_format='h5')

print_heading(f'Evaluating {best_model_path}')

!mkdir -p images

# load the saved best model
saved_model = load_model(tf_model_path)

# evaluate the model
_, train_acc = saved_model.evaluate(X_train_enc, y_train_enc, verbose=2)
_, test_acc = saved_model.evaluate(X_test_enc, y_test_enc, verbose=1)
print('Accuracy of test: %.2f' % (test_acc*100))
print('Accuracy of the: '+'1) Train: %.3f, 2) Test: %.3f' % (train_acc, test_acc))

# plot training history
plt.plot(history_3.history['loss'], label='train')
plt.plot(history_3.history['val_loss'], label='test')
plt.legend(['train', 'test'], loc='upper left')
plt.ylabel('Loss')
plt.savefig('images/training_history.png', dpi=300)
plt.show()

# summarize history for accuracy
plt.plot(history_3.history['accuracy'])
plt.plot(history_3.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.savefig('images/accuracy_history.png', dpi=300)
plt.show()

# summarize history for loss
plt.plot(history_3.history['loss'])
plt.plot(history_3.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.savefig('images/loss_history.png', dpi=300)
plt.show()

#note in kera model.predict() will return predict probabilities
pred_prob =  saved_model.predict(X_test_enc, verbose=0)
fpr, tpr, threshold = metrics.roc_curve(y_test_enc.ravel(), pred_prob.ravel())
roc_auc = metrics.auc(fpr, tpr)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(3):
    fpr[i], tpr[i], _ = metrics.roc_curve(y_test_enc[:,i], pred_prob[:, i])
    roc_auc[i] = metrics.auc(fpr[i], tpr[i])    
   
# Compute micro-average ROC curve and ROC area
fpr['micro'], tpr['micro'], _ = metrics.roc_curve(y_test_enc.ravel(), pred_prob.ravel())
roc_auc['micro'] = metrics.auc(fpr['micro'], tpr['micro'])

# Compute macro-average ROC curve and ROC area
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(3)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(3):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

# Finally average it and compute AUC
mean_tpr /= 3

fpr['macro'] = all_fpr
tpr['macro'] = mean_tpr
roc_auc['macro'] = metrics.auc(fpr['macro'], tpr['macro'])

plt.figure(1)
plt.plot(fpr['micro'], tpr['micro'],
         label='micro-average ROC curve (area = {0:0.2f})' \
               ''.format(roc_auc['micro']),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr['macro'], tpr['macro'],
         label='macro-average ROC curve (area = {0:0.2f})' \
               ''.format(roc_auc['macro']),
         color='navy', linestyle=':', linewidth=4)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'blue'])
for i, color in zip(range(3), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})' \
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Result for Receiver operating characteristic to multi-class of Reaction Time')
plt.legend(loc='lower right')
plt.savefig('images/roc.png', dpi=300)
plt.show()

"""## Create Verification Artifacts"""

def compute_nnet_params(model_file, df, scaler):
    outputs = df['TOT']
    inputs = df.drop(['Name', 'TOT', 'ReactionTime'], axis=1)
    enc_inputs = pd.DataFrame(scaler.transform(inputs.values), columns=inputs.columns)

    # compute sdev, mins, and maxs for inputs
    input_sdev = enc_inputs.std().to_numpy()
    input_mins = enc_inputs.min().to_numpy()
    input_maxs = enc_inputs.max().to_numpy()

    # extend input maxs and mins by std dev
    input_mins -= input_sdev
    input_maxs += input_sdev

    # maraboupy only supports normalization (not standardization)
    # use mean=0, and range=1 to neutralize maraboupy normalization
    means = np.zeros(enc_inputs.shape[1]+1, dtype=int) 
    ranges = np.ones(enc_inputs.shape[1]+1, dtype=int)

    # extract weights and biases from model
    model = load_model(model_file)
    model_params = model.get_weights()
    weights = [w.T for w in model_params[0:len(model_params):2]]
    biases  = model_params[1:len(model_params):2]

    return (weights, biases, input_mins, input_maxs, means, ranges)


def save_nnet(weights, biases, input_mins, input_maxs, means, ranges, output_path):
    # write model in nnet format.
    writeNNet(weights, biases, input_mins, input_maxs, means, ranges, output_path)

def save_info(weights, biases, input_mins, input_maxs, means, ranges, feature_names, tot_bins, tot_labels, output_path):
    def section(param_name, values):
        return ('-' * 40) + '\n' + f'{param_name}:\n{values}\n'
    
    tot_categories = '\n'.join([f'{tot_labels[i]}: ({tot_bins[i]}, {tot_bins[i+1]})' for i in range(len(tot_bins)-1)])
    input_nodes = ', '.join([f'{i}:{feature_names[i]}' for i in range(len(feature_names))])
    with open(output_path, 'w') as f:
        f.writelines([section('FEATURE_NAMES', feature_names),
                      section('INPUT_NODES', input_nodes),
                      section('TOT_CATEGORIES', tot_categories),
                      section('WEIGHTS', weights),
                      section('BIASES', biases),
                      section('MINS', input_mins),
                      section('MAXS', input_maxs),
                      section('MEANS', means),
                      section('RANGES', ranges)])

def save_encoders(scaler, onehot, output_dir):
    pkl.dump(scaler, open(f'{output_dir}/scaler.pkl', 'wb'))
    pkl.dump(onehot, open(f'{output_dir}/onehot.pkl', 'wb'))

def create_verification_artifacts(tf_model_path, h5_model_path, df, feature_names, tot_bins, tot_labels, scaler, onehot):
    print_heading(f'Creating verification artifacts...')
    output_dir='artifacts'
    archive_path = 'artifacts.zip'
    h5_path = os.path.join(output_dir, 'model.h5')
    pb_path = os.path.join(output_dir, 'model.pb')
    nnet_path = os.path.join(output_dir, 'model.nnet')
    info_path = os.path.join(output_dir, 'model.info')
    model_zip = os.path.join(output_dir, 'model.zip')

    # clear previous folder
    !rm -rf $output_dir
    # create the folder
    !mkdir -p $output_dir

    # zip up the tf model, and copy to artifacts
    !cd $tf_model_path/.. && zip -qr ../$model_zip model && cd - > /dev/null
    # copy the pb model file
    !cp $tf_model_path/saved_model.pb $pb_path
    # copy the h5 model file
    !cp $h5_model_path $h5_path
    # copy the images to artifacts
    !cp -r images $output_dir

    # extract params for nnet format
    nnet_params = compute_nnet_params(tf_model_path, df, scaler)
    weights, biases, input_mins, input_maxs, means, ranges = nnet_params
    # write the model to nnet file.
    save_nnet(weights, biases, input_mins, input_maxs, means, ranges, nnet_path)
    # write encoders to file
    save_encoders(scaler, onehot, output_dir)
    # save extra model information
    save_info(weights, biases, input_mins, input_maxs, means, ranges, feature_names, tot_bins, tot_labels, info_path)
    # create a zip archive of artifacts
    !zip -rq $archive_path $output_dir
    print_message(f'Saved artifacts to {archive_path}')

# create artifacts used for verification
create_verification_artifacts(tf_model_path, h5_model_path, df, feature_names, tot_bins, tot_labels, scaler, onehot)

"""## Save Model & Verification Artifacts to GDrive"""

# # GDrive ID's point to files in models/latest folder
# artifacts = {
#     'artifacts/model.info': '1l59_JttOSA4rwy2a4-fnPkezfIoaULXp', # info file
#     'artifacts/model.zip': '100s5DVwaK6ILlDe2ZCgm2F8JGrY7Wixf',  # tf format
#     'artifacts/model.h5': '1Kyxb1A4E6U_HPaPjRLVnb2OTJtXOzTXX',   # h5 format
#     'artifacts/model.pb': '1Ap3eWHWwAyw_3wOmy237AJF3pWQRnG3_',   # pb format
#     'artifacts/model.nnet': '1HzfGxhKrw9PpeA1cMsexC4FcWv5OPdtB', # nnet format
#     'artifacts/scaler.pkl': '10EkqHQ3aqEYAxbLS4Q4LRWJ1byNCvAcf', # scaler object
#     'artifacts/onehot.pkl': '1SeED9m_TeyqtmHRgDe_kd9HVmn2K1hh8'  # onehot object
#     }

# # upload all of the artifacts to drive
# for fname,driveid in artifacts.items():
#     overwrite_gdrive_file(driveid, fname)

# # Test artifact IDs
# test_artifacts = {
#     'artifacts/model.info': '1ukfM8reEtyWBkcTqLyNIWqJUtYEZlE3W',
#     'artifacts/model.zip': '1foS3Gotcr72_J7PBwVNnap54sDKBRTXa',
#     'artifacts/model.h5': '1Km81P7I5zhyIpdhfG-FvEtQoak9-vDMR',
#     'artifacts/model.pb': '1iY72ZMW9GVe2xGwkvWzeXL6exXi0tIAk',
#     'artifacts/model.nnet': '1wrtH7fhKJgisDScJZKXH2WlkhpV5NKJR',
#     'artifacts/scaler.pkl': '14ydD-cpE41xaHUSPD4hzJlFjWQc7l4uG',
#     'artifacts/onehot.pkl': '1FVoFFg-OfvF1V9BRlNuRfSNJNHcNumO7'
#     }
# for fname,driveid in test_artifacts.items():
#     overwrite_gdrive_file(driveid, fname)

"""## Visualization"""

display(tf.keras.utils.plot_model(model, to_file='images/model.png', show_shapes=True, show_layer_names=True))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $tensorboard_logs --host localhost --port 6006